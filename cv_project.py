# -*- coding: utf-8 -*-
"""CV  project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gf4kD0u6qIB85N0ksQbKIBJ-iK6g1zFg
"""

from google.colab import drive
drive.mount('/content/drive')

data_path = "/content/drive/MyDrive/Dataset"

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras import layers, models
import random

labels_file_path = os.path.join(data_path, "labels.txt")
with open(labels_file_path, "r") as f:
    labels = f.read().splitlines()

# Contents of the "train" and "test" folders for each class
for folder_name in ["train", "test"]:
    folder_path = os.path.join(data_path, folder_name)
    print(f"Contents of {folder_name} folder:")
    for label in labels:
        label_folder_path = os.path.join(folder_path, label)
        image_list = os.listdir(label_folder_path)
        print(f"Class: {label}, Number of images: {len(image_list)}")

data = []
target = []

print("data_path:", data_path)
print("label_folder_path:", label_folder_path)
print("image_list:", image_list)

# Step 1: Accessing the Dataset from Google Drive (For Google Colab)
from google.colab import drive
drive.mount('/content/drive')
data_path = "/content/drive/MyDrive/Dataset"  # Set the correct path to the "Dataset" folder

# Step 2: Check the number of images in each class folder (Train Set)
train_folder_path = os.path.join(data_path, "train")
print("Contents of train folder:")
for label in os.listdir(train_folder_path):
    label_folder_path = os.path.join(train_folder_path, label)
    num_images = len(os.listdir(label_folder_path))
    print(f"Class: {label}, Number of images: {num_images}")

# Step 3: Check the number of images in each class folder (Test Set)
test_folder_path = os.path.join(data_path, "test")
print("Contents of test folder:")
for label in os.listdir(test_folder_path):
    label_folder_path = os.path.join(test_folder_path, label)
    num_images = len(os.listdir(label_folder_path))
    print(f"Class: {label}, Number of images: {num_images}")

import matplotlib.pyplot as plt
# Step 2: Load and Preprocess the Data
labels_file_path = os.path.join(data_path, "labels.txt")
with open(labels_file_path, "r") as f:
    labels = f.read().splitlines()
# Step 3: Loop through each class and print one image
for folder_name in ["train", "test"]:
    folder_path = os.path.join(data_path, folder_name)
    for label in labels:
        label_folder_path = os.path.join(folder_path, label)
        for img_name in os.listdir(label_folder_path):
            img_path = os.path.join(label_folder_path, img_name)
            img = cv2.imread(img_path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed
            img = cv2.resize(img, (64, 64))  # Resize the image to the desired input size
            data.append(img)
            target.append(labels.index(label))

data = np.array(data)  # Convert to NumPy array
target = np.array(target)

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)

X_train

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(7, activation='softmax'))  # 7 classes for facial expressions

model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

X_train = X_train.reshape((-1, 64, 64, 1))  # Add an extra dimension for the channel (grayscale)
X_test = X_test.reshape((-1, 64, 64, 1))
X_train = X_train.astype('float32') / 255.0  # Normalize the pixel values between 0 and 1
X_test = X_test.astype('float32') / 255.0

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test accuracy:", test_accuracy)

predictions = model.predict(X_test)

predictions

predicted_labels = np.argmax(predictions, axis=1)

print(predicted_labels)

# Step 2: Load and Preprocess the Data (Only for visualization)
labels_file_path = os.path.join(data_path, "labels.txt")
with open(labels_file_path, "r") as f:
    labels = f.read().splitlines()

# Visualization for the "train" folder
train_folder_path = os.path.join(data_path, "train")
for label in labels:
    label_folder_path = os.path.join(train_folder_path, label)
    image_list = os.listdir(label_folder_path)
    if len(image_list) > 0:
        img_name = random.choice(image_list)
        img_path = os.path.join(label_folder_path, img_name)
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for correct visualization
        print(f"Train - Class: {label}")
        plt.imshow(img)
        plt.axis('off')
        plt.show()
    else:
        print(f"No images found in the {label} class folder in the train set.")

# Visualization for the "test" folder
test_folder_path = os.path.join(data_path, "test")
for label in labels:
    label_folder_path = os.path.join(test_folder_path, label)
    image_list = os.listdir(label_folder_path)
    if len(image_list) > 0:
        img_name = random.choice(image_list)
        img_path = os.path.join(label_folder_path, img_name)
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for correct visualization
        print(f"Test - Class: {label}")
        plt.imshow(img)
        plt.axis('off')
        plt.show()
    else:
        print(f"No images found in the {label} class folder in the test set.")